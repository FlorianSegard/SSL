{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlorianSegard/SSL/blob/main/TP_VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Variational Autoencoder (VAE) with Latent Space Interpolation on MNIST**\n",
        "\n",
        "## Project Description\n",
        "\n",
        "In this practical assignment, we explore the concepts of **Variational Autoencoders (VAEs)** by implementing and training a convolutional VAE on the MNIST dataset. The primary objectives are to learn how VAEs encode data into a structured latent space and to investigate how this latent space can be leveraged for generating new data and understanding data features.\n",
        "\n",
        "### Key Learning Goals:\n",
        "1. **Data Loading and Preprocessing**: Load the MNIST dataset and prepare it for training a convolutional neural network.\n",
        "2. **VAE Architecture**: Define a VAE model with convolutional layers for encoding and decoding images, including a reparameterization trick to ensure smooth sampling from the latent space.\n",
        "3. **VAE Loss Function**: Understand and implement the VAE loss, which combines reconstruction loss and KL-divergence to balance accurate reconstructions with a regularized latent space.\n",
        "4. **Model Training**: Train the VAE model on MNIST, observe loss trends, and understand the impact of balancing reconstruction quality and latent space regularization.\n",
        "5. **Latent Space Visualization**: Perform latent space interpolation by linearly blending between two points in the latent space, generating smooth transitions between two digit classes."
      ],
      "metadata": {
        "id": "W7Q8uwOq0X-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Loading the MNIST Dataset\n",
        "\n",
        "In this first step, we load the MNIST dataset, which contains 28x28 grayscale images of handwritten digits (0-9). Each image represents a single digit.\n",
        "\n",
        "We use PyTorch's `torchvision.datasets` and `DataLoader` to load and preprocess the data efficiently.\n",
        "\n",
        "### Code Breakdown:\n",
        "- **Transforms**: `transforms.Compose([transforms.ToTensor()])` converts each image into a tensor format. This allows the data to be used in PyTorch models.\n",
        "- **Dataset**: `datasets.MNIST` loads the MNIST dataset and applies the specified transform. We set `download=True` to ensure the data is downloaded if it hasn't been already.\n",
        "- **DataLoader**: The `DataLoader` wraps the dataset and enables batching, shuffling, and parallel loading.\n",
        "\n",
        "This setup provides a `train_loader`, which we will use to feed batches of images into our model during training."
      ],
      "metadata": {
        "id": "kfrpIiPJx7nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Loading MNIST data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "liyE44fBkzwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df68a814-02f8-4148-eabb-59df438e2fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 497kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.88MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.15MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining the Convolutional VAE Model\n",
        "\n",
        "In this section, we define the architecture of a **Variational Autoencoder (VAE)** with convolutional layers, designed to process the MNIST dataset.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Initialization and Latent Dimension**:\n",
        "   - `latent_dim=2` is defined as the dimensionality of the latent space. Setting it to a low number (e.g., 2) makes the latent space easy to visualize.\n",
        "\n",
        "2. **Encoder**:\n",
        "   - The encoder consists of three convolutional layers:\n",
        "     - The first layer transforms the input image of size 1x28x28 to a feature map of size 32x14x14.\n",
        "     - The second layer further reduces it to a size of 64x7x7.\n",
        "     - The third layer compresses this to a size of 128x1x1.\n",
        "   - This results in a flattened feature vector, which is then fed into two fully connected layers (`fc_mu` and `fc_logvar`) to produce the **mean** (`mu`) and **log-variance** (`logvar`) of the latent space.\n",
        "\n",
        "3. **Reparameterization Trick**:\n",
        "   - In `reparameterize`, we use the mean and log-variance vectors to sample from a Gaussian distribution. This is done by computing:\n",
        "\n",
        "     z = \\mu + \\sigma \\cdot \\epsilon\n",
        "\n",
        "   - Here, `epsilon` is a random noise sampled from a standard normal distribution. This allows gradients to backpropagate through the sampling process.\n",
        "\n",
        "4. **Decoder**:\n",
        "   - The decoder starts with a fully connected layer to expand the latent vector (`z`) back to a shape compatible with the convolutional layers.\n",
        "   - It then passes through three transposed convolutional layers to reconstruct the original image shape:\n",
        "     - The first layer reshapes it to 64x7x7.\n",
        "     - The second layer outputs 32x14x14.\n",
        "     - The final layer produces the original shape, 1x28x28, with pixel values normalized between 0 and 1 using a Sigmoid activation.\n",
        "\n",
        "5. **Forward Pass**:\n",
        "   - `encode`: Passes the input through the encoder to obtain `mu` and `logvar`.\n",
        "   - `reparameterize`: Samples from the latent distribution using `mu` and `logvar`.\n",
        "   - `decode`: Reconstructs the image from the latent vector `z`.\n",
        "   - Returns the reconstructed image along with `mu` and `logvar` for further use in the loss calculation.\n",
        "\n",
        "This architecture allows the VAE to encode images to a low-dimensional latent space and then decode them back to their original shape."
      ],
      "metadata": {
        "id": "68vTRMlqyLte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNSNUz9kktWX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2):\n",
        "        super(ConvVAE, self).__init__()\n",
        "\n",
        "        # Encoder with convolutional layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Input 1x28x28 -> Output 32x14x14\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Input 32x14x14 -> Output 64x7x7\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Input 64x7x7 -> Output 128x1x1\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "\n",
        "            nn.Linear(128*7*7, 128),\n",
        "        )\n",
        "\n",
        "        # Layers to produce mean and variance vectors\n",
        "        # Mean vector of the latent space\n",
        "        self.fc_mu = nn.Linear(128, latent_dim)\n",
        "\n",
        "        # Log-variance vector of the latent space\n",
        "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "\n",
        "        # Decoder starting with a fully connected layer\n",
        "        # Transform latent vector back to decoder shape\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.Unflatten(1, (128, 1, 1)),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=7, stride=2),\n",
        "            nn.Tanh(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=8),\n",
        "            nn.Sigmoid(),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=15),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        # Decoder with transposed convolutional layers to reconstruct the image\n",
        "        # Input 128x1x1 -> Output 64x7x7\n",
        "\n",
        "        # Input 64x7x7 -> Output 32x14x14\n",
        "\n",
        "        # Input 32x14x14 -> Output 1x28x28\n",
        "\n",
        "        # Normalize output pixels between 0 and 1\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Apply the encoder to extract features and flatten the result\n",
        "        x = self.encoder(x)\n",
        "        # Compute mean and log-variance for the latent space distribution\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # Compute standard deviation from log-variance\n",
        "        std = torch.exp(logvar)\n",
        "\n",
        "        # Sample epsilon from a normal distribution\n",
        "        eps = torch.randn_like(mu)\n",
        "\n",
        "        # Reparameterization trick to sample z from N(mu, sigma^2)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Apply fully connected layer to expand latent vector to decoder's initial shape\n",
        "\n",
        "\n",
        "        # Apply the decoder to reconstruct the image\n",
        "        # return self.decoder(h)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through encoder\n",
        "        mu, logvar = self.encode(x)\n",
        "        # Sample from latent distribution\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        # Forward pass through decoder\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = ConvVAE().cuda()\n",
        "summary(model, (1,28,28))"
      ],
      "metadata": {
        "id": "CBwqVgsv2keT",
        "outputId": "e2767f75-eacc-4c04-e472-da1e4f6460af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "              Tanh-2           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
            "            Conv2d-4           [-1, 64, 14, 14]          18,496\n",
            "           Sigmoid-5           [-1, 64, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
            "            Conv2d-7            [-1, 128, 7, 7]          73,856\n",
            "              Tanh-8            [-1, 128, 7, 7]               0\n",
            "           Flatten-9                 [-1, 6272]               0\n",
            "           Linear-10                  [-1, 128]         802,944\n",
            "           Linear-11                    [-1, 2]             258\n",
            "           Linear-12                    [-1, 2]             258\n",
            "           Linear-13                  [-1, 128]             384\n",
            "        Unflatten-14            [-1, 128, 1, 1]               0\n",
            "  ConvTranspose2d-15             [-1, 64, 7, 7]         401,472\n",
            "             Tanh-16             [-1, 64, 7, 7]               0\n",
            "  ConvTranspose2d-17           [-1, 32, 14, 14]         131,104\n",
            "          Sigmoid-18           [-1, 32, 14, 14]               0\n",
            "  ConvTranspose2d-19            [-1, 1, 28, 28]           7,201\n",
            "          Sigmoid-20            [-1, 1, 28, 28]               0\n",
            "================================================================\n",
            "Total params: 1,436,293\n",
            "Trainable params: 1,436,293\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.95\n",
            "Params size (MB): 5.48\n",
            "Estimated Total Size (MB): 6.43\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Defining the VAE Loss Function\n",
        "\n",
        "The Variational Autoencoder (VAE) loss function combines two key components: **Reconstruction Loss** and **KL-Divergence Loss**. Together, these terms encourage the VAE to produce high-quality reconstructions while also regularizing the latent space.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Flattening**:\n",
        "   - Both `recon_x` (the reconstructed image) and `x` (the original image) are flattened to a shape of `[batch_size, 784]` to match the expected input shape for the binary cross-entropy function.\n",
        "\n",
        "2. **Reconstruction Loss (Binary Cross-Entropy)**:\n",
        "   - We use Binary Cross-Entropy (BCE) as the reconstruction loss. This term measures the pixel-wise difference between the original and reconstructed images.\n",
        "   - `BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')` sums the error over all pixels in the batch, promoting accurate reconstructions.\n",
        "\n",
        "3. **KL-Divergence Loss**:\n",
        "   - The KL-Divergence term measures the difference between the learned latent distribution `q(z|x)` and a standard normal distribution `p(z) = N(0, 1)`.\n",
        "   - The calculation:\n",
        "\\[\n",
        "     KLD = -0.5 \\sum (1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2)\n",
        "\\]\n",
        "   - This term penalizes deviations from the standard normal distribution, helping to structure the latent space and ensure continuity.\n",
        "\n",
        "4. **Total VAE Loss**:\n",
        "   - The final loss is the sum of the reconstruction loss (BCE) and the KL-Divergence loss (KLD). Minimizing this total loss encourages the model to reconstruct images accurately while keeping the latent space organized.\n",
        "\n",
        "By using this combined loss, the VAE learns to generate images that resemble the input data and maintain a well-structured latent space that facilitates tasks like image generation and interpolation."
      ],
      "metadata": {
        "id": "XXxPhUHey9fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # Flatten the output to match the shape of x\n",
        "    torch.flatten(recon_x, start_dim=1)\n",
        "\n",
        "    # Also flatten x for consistency\n",
        "    torch.flatten(x, start_dim=1)\n",
        "\n",
        "    # Reconstruction loss: Binary Cross Entropy\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # KL-Divergence loss\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar))\n",
        "\n",
        "    # Total VAE loss\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "rmNTJwtSlMEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training the VAE Model\n",
        "\n",
        "In this step, we set up and execute the training loop for the Variational Autoencoder (VAE) model. The training process optimizes the model’s parameters to minimize the combined loss function over multiple epochs.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Setting up the Device**:\n",
        "   - The code detects if a GPU is available using `torch.cuda.is_available()`. If so, it uses the GPU for faster training; otherwise, it defaults to the CPU.\n",
        "\n",
        "2. **Model and Optimizer Initialization**:\n",
        "   - We create an instance of the `ConvVAE` model with a specified `latent_dim`. A small latent dimension (like 2) makes it easier to visualize the latent space later.\n",
        "   - The model is sent to the chosen device (CPU or GPU).\n",
        "   - The optimizer is set up using Adam with a learning rate of 0.001, which is well-suited for training VAEs.\n",
        "\n",
        "3. **Training Loop**:\n",
        "   - `num_epochs` defines the number of times the entire dataset is processed during training.\n",
        "   - For each epoch:\n",
        "     - The model is set to training mode (`model.train()`), which activates features like dropout (if used).\n",
        "     - `train_loss` is initialized to accumulate the total loss over the epoch.\n",
        "     - For each batch in `train_loader`:\n",
        "       - **Data Transfer**: The batch of images is transferred to the chosen device.\n",
        "       - **Gradient Reset**: `optimizer.zero_grad()` resets gradients from the previous batch to prevent accumulation.\n",
        "       - **Forward Pass**: The data is passed through the model, which outputs `recon_batch` (reconstructed images), `mu` (mean), and `logvar` (log-variance) of the latent distribution.\n",
        "       - **Loss Calculation**: `loss_function` computes the VAE loss by combining reconstruction and KL-divergence losses.\n",
        "       - **Backpropagation**: `loss.backward()` computes the gradients for all model parameters.\n",
        "       - **Optimization Step**: `optimizer.step()` updates the parameters using the calculated gradients.\n",
        "       - The batch loss is added to `train_loss` to track the total loss for the epoch.\n",
        "   \n",
        "   - After each epoch, the average loss for that epoch is printed for monitoring progress. This average loss helps to assess if the model is converging.\n",
        "\n",
        "The training loop fine-tunes the VAE’s parameters to generate accurate reconstructions and a well-structured latent space, which can be visualized or used for generative tasks after training."
      ],
      "metadata": {
        "id": "jH_iNES9zVHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, optimizer, and send model to device\n",
        "latent_dim = 2\n",
        "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        data = data.cuda()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the VAE\n",
        "        recon_data, mu, logvar = model(data)\n",
        "\n",
        "        # Calculate the VAE loss\n",
        "        loss = loss_function(recon_data, data, mu, logvar)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the average loss for each epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}')"
      ],
      "metadata": {
        "id": "Zoea4byXlOo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44000db0-b12d-4040-c398-e2a3323d4566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 228.1483\n",
            "Epoch 2/10, Loss: 179.8804\n",
            "Epoch 3/10, Loss: 169.0876\n",
            "Epoch 4/10, Loss: 166.8273\n",
            "Epoch 5/10, Loss: 163.0419\n",
            "Epoch 6/10, Loss: 161.8082\n",
            "Epoch 7/10, Loss: 160.9248\n",
            "Epoch 8/10, Loss: 160.3261\n",
            "Epoch 9/10, Loss: 159.2543\n",
            "Epoch 10/10, Loss: 158.1258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Interpolation in the Latent Space\n",
        "\n",
        "In this final step, we perform **latent space interpolation** to explore the continuity of the VAE's learned latent space. By interpolating between two points in the latent space (representing different digits), we can generate a smooth transition between them.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Defining the Interpolation Function**:\n",
        "   - `interpolate_and_generate`: This function takes two latent vectors (`z_start` and `z_end`) and generates intermediate points between them.\n",
        "   - **Interpolation**: We create a series of interpolated points by linearly blending `z_start` and `z_end` with interpolation weights `t` ranging from 0 to 1.\n",
        "   - **Decoding**: Each interpolated vector is decoded by the VAE’s decoder to produce an image, which is stored in the `images` list.\n",
        "\n",
        "2. **Selecting Points to Interpolate**:\n",
        "   - We randomly select two samples from the dataset (e.g., images of \"1\" and \"7\").\n",
        "   - These images are passed through the encoder to obtain their latent representations, `z_start` and `z_end`.\n",
        "\n",
        "3. **Generating and Visualizing the Interpolation**:\n",
        "   - The interpolated latent points are decoded back into images.\n",
        "   - We then plot each decoded image side-by-side to visualize the transformation from the starting digit to the ending digit.\n",
        "   \n",
        "   Each intermediate image represents a gradual change in the latent space between `z_start` and `z_end`, showing how the model \"morphs\" one digit into another.\n",
        "\n",
        "### Visualization Explanation:\n",
        "\n",
        "The visualization showcases the VAE’s ability to generate new images by sampling from the latent space. The smooth transition between digits demonstrates that the VAE has learned a well-structured, continuous latent space where similar concepts (like digits) are located close to each other. This capability can be useful for generating synthetic data or exploring variations in features.\n",
        "\n",
        "By interpolating in this manner, we can observe how the VAE understands and generates the fundamental features of the digits in the MNIST dataset."
      ],
      "metadata": {
        "id": "m8HSV3JuzqD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the interpolation function\n",
        "def interpolate_and_generate(model, z_start, z_end, steps=10):\n",
        "    # Create interpolation steps between two latent vectors\n",
        "    # Decode each interpolated point and store the images\n",
        "    images = []\n",
        "    for t in np.linspace(0, 1, steps):\n",
        "        z = (1 - t) * z_start + t * z_end\n",
        "        image = model.decode(z.to(device))\n",
        "        images.append(image.detach().cpu().numpy())\n",
        "    return images\n",
        "\n",
        "# Select two random digits to interpolate between\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Pass the data directly without reshaping, as the model expects [1, 1, 28, 28]\n",
        "    # Randomly select two samples from the dataset for interpolation\n",
        "    num_samples = 2\n",
        "    steps = 100\n",
        "    random_indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "    sample1, _ = train_dataset[random_indices[0]]\n",
        "    sample2, _ = train_dataset[random_indices[1]]\n",
        "\n",
        "    # Encode the samples into the latent space\n",
        "    z_start, _ = model.encode(sample1.unsqueeze(0).to(device))\n",
        "    z_end, _ = model.encode(sample2.unsqueeze(0).to(device))\n",
        "\n",
        "    # Generate the interpolation sequence\n",
        "    interpolated_images = interpolate_and_generate(model, z_start, z_end, steps=steps)\n",
        "\n",
        "# Plot the interpolated images\n",
        "# Create a figure and subplots with 10 rows and 10 columns\n",
        "fig, axes = plt.subplots(10, 10, figsize=(15, 15)) # Change figsize for better visualization\n",
        "\n",
        "# Iterate through the interpolated images and display them in the subplots\n",
        "for i in range(10):  # Iterate through rows\n",
        "    for j in range(10):  # Iterate through columns\n",
        "        image_index = i * 10 + j  # Calculate the index of the image to display\n",
        "        axes[i, j].imshow(interpolated_images[image_index][0, 0], cmap='gray')\n",
        "        axes[i, j].axis('off')  # Turn off axis labels\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "seg0fyNyntjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}